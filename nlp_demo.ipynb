{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "8wrU9xiiN6WH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a953b3e3-f35a-40d5-d217-f2f6c6eedf8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lime in /usr/local/lib/python3.10/dist-packages (0.2.0.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lime) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lime) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lime) (1.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lime) (4.66.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from lime) (1.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime) (0.19.3)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (23.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (3.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.104.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.1)\n",
            "Requirement already satisfied: gradio-client==0.7.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.7.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.19.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.9.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.5.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.6)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.31.0)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.8.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.0.post1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.7.0->gradio) (2023.6.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.7.0->gradio) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.13.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.14.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2023.7.22)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (3.7.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.27.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.0.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.31.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.13.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Requirement already satisfied: skipthoughts in /usr/local/lib/python3.10/dist-packages (0.0.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from skipthoughts) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from skipthoughts) (1.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->skipthoughts) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->skipthoughts) (4.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->skipthoughts) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->skipthoughts) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->skipthoughts) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->skipthoughts) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->skipthoughts) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->skipthoughts) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->skipthoughts) (1.3.0)\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2023.7.22)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "!pip install lime\n",
        "!pip install gradio\n",
        "!pip install skipthoughts\n",
        "!pip install vaderSentiment\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.util import ngrams\n",
        "import pickle\n",
        "from torch.autograd import Variable\n",
        "import sys\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from sklearn import feature_extraction\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "iuqM5AAyCUb2"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npMdYRKVOVhg",
        "outputId": "539fb80a-7c5f-4f5f-870e-efc0d22cff9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "dkfaT77ROesD"
      },
      "outputs": [],
      "source": [
        "my_dict = pickle.load(open('gdrive/MyDrive/Data/my_dict.pkl', 'rb'))\n",
        "biskip = pickle.load(open('gdrive/MyDrive/Data/biskip.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "EmOhwXvVOv7u"
      },
      "outputs": [],
      "source": [
        "def neural_features(headlines,bodies):\n",
        "  MAX_HEAD = max(len(ele) for ele in headlines)\n",
        "  MAX_BODY = max(len(ele) for ele in bodies)\n",
        "  headlines_to_ids = np.zeros((len(headlines),MAX_HEAD+1))\n",
        "  bodies_to_ids = np.zeros((len(bodies),MAX_BODY+1))\n",
        "  headlines_encodings = np.zeros((len(headlines),2400))\n",
        "  bodies_encodings = np.zeros((len(bodies),2400))\n",
        "  for i in range(len(headlines)):\n",
        "    headline = headlines[i].split()\n",
        "    headline.append('<eos>')\n",
        "    body = bodies[i].split()\n",
        "    body.append('<eos>')\n",
        "    j=0\n",
        "    for word in headline:\n",
        "      try:\n",
        "        headlines_to_ids[i][j] = my_dict[word]\n",
        "      except KeyError:\n",
        "        pass\n",
        "      j+=1\n",
        "    j=0\n",
        "    for word in body:\n",
        "      try:\n",
        "        bodies_to_ids[i][j] = my_dict[word]\n",
        "      except KeyError:\n",
        "        pass\n",
        "      j+=1\n",
        "  last_temp = len(headlines) - len(headlines)%50\n",
        "  for i in range(0,len(headlines),50):\n",
        "    # print(i)\n",
        "    input1 = Variable(torch.LongTensor(headlines_to_ids[i:i+50]))\n",
        "    input2 = Variable(torch.LongTensor(bodies_to_ids[i:i+50]))\n",
        "    headline_output = biskip(input1).detach().numpy()\n",
        "    body_output = biskip(input2).detach().numpy()\n",
        "    headlines_encodings[i:i+50] = headline_output[0:50]\n",
        "    bodies_encodings[i:i+50] = body_output[0:50]\n",
        "  if(last_temp != len(headlines)):\n",
        "    input1 = Variable(torch.LongTensor(headlines_to_ids[last_temp:]))\n",
        "    input2 = Variable(torch.LongTensor(bodies_to_ids[last_temp:]))\n",
        "    headline_output = biskip(input1).detach().numpy()\n",
        "    body_output = biskip(input2).detach().numpy()\n",
        "    headlines_encodings[last_temp:] = headline_output[:]\n",
        "    bodies_encodings[last_temp:] = body_output[:]\n",
        "\n",
        "  feat1 = np.zeros((len(headlines),2400))\n",
        "  feat2 = np.zeros((len(headlines),2400))\n",
        "  i = 0\n",
        "  for h_vector,b_vector in zip(headlines_encodings,bodies_encodings):\n",
        "    feat1[i] = np.multiply(h_vector,b_vector)\n",
        "    feat2[i] = np.absolute(h_vector-b_vector)\n",
        "    i+=1\n",
        "\n",
        "  final_neural_features = np.concatenate((feat1,feat2),axis = 1)\n",
        "  return final_neural_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "efKc9kKbSAeN"
      },
      "outputs": [],
      "source": [
        "\n",
        "_wnl = nltk.WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def normalize_word(w):\n",
        "    return _wnl.lemmatize(w).lower()\n",
        "\n",
        "\n",
        "def get_tokenized_lemmas(s):\n",
        "    return [normalize_word(t) for t in nltk.word_tokenize(s)]\n",
        "\n",
        "\n",
        "def clean(s):\n",
        "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
        "\n",
        "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
        "\n",
        "\n",
        "def remove_stopwords(l):\n",
        "    # Removes stopwords from a list of tokens\n",
        "    return [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS]\n",
        "\n",
        "def preprocess(headlines,bodies):\n",
        "  n_headlines, n_bodies =[],[]\n",
        "  for i, (headline, body) in enumerate(zip(headlines, bodies)):\n",
        "    clean_headline = clean(headline)\n",
        "    clean_body = clean(body)\n",
        "    clean_headline = get_tokenized_lemmas(clean_headline)\n",
        "    clean_body = get_tokenized_lemmas(clean_body)\n",
        "    clean_headline = remove_stopwords(clean_headline)\n",
        "    clean_body = remove_stopwords(clean_body)\n",
        "    n_headlines.append(headline)\n",
        "    n_bodies.append(body)\n",
        "  n_headlines_df=pd.DataFrame(n_headlines,columns=['Headline'])\n",
        "  n_bodies_df=pd.DataFrame(n_bodies,columns=['Body'])\n",
        "  return n_headlines_df['Headline'], n_bodies_df['Body']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "uNxEjVkgSG_A"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('gdrive/MyDrive/Data/train_Set.csv')\n",
        "org_hs, org_bs = df['Headline'], df['Body']\n",
        "org_hs,org_bs = preprocess(org_hs,org_bs)\n",
        "headline_vectorizer = TfidfVectorizer()\n",
        "h1 = headline_vectorizer.fit(org_hs)\n",
        "def statistical_features(headlines,bodies):\n",
        "  # stop_words_l=stopwords.words('english')\n",
        "  # org_hs, org_bs = df['Headline'], df['Body']\n",
        "  # org_hs,org_bs = preprocess(org_hs,org_bs)\n",
        "  # headline_vectorizer = TfidfVectorizer()\n",
        "  # h1 = headline_vectorizer.fit(org_hs)\n",
        "  h = h1.transform(headlines)\n",
        "  body_vectorizer = TfidfVectorizer(max_features=10000-h.shape[1])\n",
        "  b1 = body_vectorizer.fit(org_bs)\n",
        "  b = b1.transform(bodies)\n",
        "  statistical_features = np.concatenate((np.array(h.toarray()),np.array(b.toarray())),axis = 1)\n",
        "  return statistical_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "mz-B3jRZYgBe"
      },
      "outputs": [],
      "source": [
        "def external_features(headline,body):\n",
        "  eng_ext = []\n",
        "  i = 0\n",
        "  for sent1,sent2 in zip(headline,body):\n",
        "    i+=1\n",
        "    vec = []\n",
        "\n",
        "    #character ngrams\n",
        "    for n in range(2,17):\n",
        "      n_grams_1 = ngrams(sent1.lower(), n)\n",
        "      n_grams_2 = ngrams(sent2.lower(),n)\n",
        "      vec.append(len(list(set(n_grams_1).intersection(set(n_grams_2)))))\n",
        "      temp_c1=0\n",
        "      temp_c2=0\n",
        "      n_grams_1 = ngrams(sent1.lower(), n)\n",
        "      n_grams_3 = ngrams(sent2.lower()[:255],n)\n",
        "      temp_c1 = len(list(set(n_grams_1).intersection(set(n_grams_3))))\n",
        "      n_grams_1 = ngrams(sent1.lower(), n)\n",
        "      n_grams_4 = ngrams(sent2.lower()[:100],n)\n",
        "      temp_c2 = len(list(set(n_grams_1).intersection(set(n_grams_4))))\n",
        "      vec.append(temp_c1)\n",
        "      vec.append(temp_c2)\n",
        "\n",
        "    #word ngrams\n",
        "    for n in range(2,7):\n",
        "      n_grams_1 = ngrams(sent1.lower().split(), n)\n",
        "      n_grams_2 = ngrams(sent2.lower().split(),n)\n",
        "      vec.append(len(list(set(n_grams_1).intersection(set(n_grams_2)))))\n",
        "      temp_c=0\n",
        "      n_grams_1 = ngrams(sent1.lower().split(), n)\n",
        "      n_grams_3 = ngrams(sent2.lower()[:255].split(),n)\n",
        "      temp_c=len(list(set(n_grams_1).intersection(set(n_grams_3))))\n",
        "      vec.append(temp_c)\n",
        "\n",
        "    #no of common words between headline and body with respect to total words\n",
        "    s1 = sent1.split()\n",
        "    s2 = sent2.split()\n",
        "    vec.append(len(set(s1).intersection(s2)) / float(len(set(s1).union(s2))))\n",
        "\n",
        "    sid_obj = SentimentIntensityAnalyzer()\n",
        "    d1 = sid_obj.polarity_scores(sent1)\n",
        "    d2 = sid_obj.polarity_scores(sent2)\n",
        "    vec.append(np.absolute(d1['neg']-d2['neg']))\n",
        "    vec.append(np.absolute(d1['neu']-d2['neu']))\n",
        "    vec.append(np.absolute(d1['pos']-d2['pos']))\n",
        "    vec.append(np.absolute(d1['compound']-d2['compound']))\n",
        "\n",
        "    eng_ext.append(vec)\n",
        "\n",
        "  eng_ext = np.array(eng_ext)\n",
        "  return eng_ext\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        # Define the first branch\n",
        "        self.x_model = nn.Sequential(\n",
        "            nn.Linear(4800, 500),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(500, 100),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Define the second branch\n",
        "        self.y_model = nn.Sequential(\n",
        "            nn.Linear(10000, 500),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(500, 50),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Define the third branch\n",
        "        self.z_model = nn.Sequential(\n",
        "            nn.Linear(60, 60),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Combine the three branches\n",
        "        self.fc_combined = nn.Sequential(\n",
        "            nn.Linear(100 + 50 + 60, 4),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y, z):\n",
        "        x_out = self.x_model(x)\n",
        "        y_out = self.y_model(y)\n",
        "        z_out = self.z_model(z)\n",
        "        combined = torch.cat([x_out, y_out, z_out], dim=1)\n",
        "        output = self.fc_combined(combined)\n",
        "        return output\n",
        "\n",
        "# Instantiate the model\n",
        "model = MyModel()"
      ],
      "metadata": {
        "id": "v-f9r6k1Jot4"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('gdrive/MyDrive/Data/final_model.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk5qmy4nJ-SX",
        "outputId": "534a3321-14ee-4d18-b6a1-592c8710a058"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check(headline,body):\n",
        "  headlines_org = []\n",
        "  bodies_org = []\n",
        "  headlines_org.append(headline)\n",
        "  headlines_org.append(headline)\n",
        "  bodies_org.append(body)\n",
        "  bodies_org.append(body)\n",
        "  n_feats = neural_features(headlines_org,bodies_org)\n",
        "  s_feats = statistical_features(headlines_org,bodies_org)\n",
        "  e_feats = external_features(headlines_org,bodies_org)\n",
        "  e_feats = torch.tensor(e_feats, dtype=torch.float32)\n",
        "  n_feats = torch.tensor(n_feats, dtype=torch.float32)\n",
        "  s_feats = torch.tensor(s_feats, dtype=torch.float32)\n",
        "  predicted = None\n",
        "  with torch.no_grad():\n",
        "      outputs = model(n_feats,s_feats,e_feats)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "  index_to_stance = {0: 'agree', 1: 'disagree', 2: 'discuss', 3: 'unrelated'}\n",
        "  predictions = [index_to_stance[p.item()] for p in predicted.cpu().numpy()]\n",
        "  return predictions[0]"
      ],
      "metadata": {
        "id": "orjYw9Y5jZ_5"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "head = 'ISIL Beheads American Photojournalist in Iraq' #discuss\n",
        "body = 'James Foley, an American journalist who went missing in Syria more than a year ago, has reportedly been executed by the Islamic State, a militant group formerly known as ISIS.Video and photos purportedly of Foley emerged on Tuesday. A YouTube video -- entitled \"A Message to #America (from the #IslamicState)\" -- identified a man on his knees as \"James Wright Foley,\" and showed his execution.This is a developing story. Check back here for updates.'\n",
        "check(head,body)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "DGgwxPdtkRYB",
        "outputId": "020f6818-8ed9-4386-83c3-4a4d4a2cbd92"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'discuss'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !pip install -U gradio\n",
        "# import gradio as gr\n",
        "\n",
        "# iface = gr.Interface(\n",
        "#     fn=check,\n",
        "#     inputs=[\"text\", \"text\"],\n",
        "#     outputs=\"text\",\n",
        "#     live=True\n",
        "# )\n",
        "\n",
        "# iface.launch()\n"
      ],
      "metadata": {
        "id": "aTUQ67L4m-Xd"
      },
      "execution_count": 45,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}